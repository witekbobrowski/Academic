{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (Summer 2018)\n",
    "\n",
    "## Homework 6\n",
    "\n",
    "Build a random forest based on\n",
    "`DecisionTree` class. Use the same dataset cars for testing.\n",
    "\n",
    "\n",
    "### Preparation\n",
    "Import\n",
    "packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randrange\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy-paste code from notebook that was used in classes (with minor changes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\" A basic Decision Tree\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "\n",
    "    def classify(self, tree, datapoint):\n",
    "\n",
    "        if type(tree) == type(\"string\"):\n",
    "            # Have reached a leaf\n",
    "            return tree\n",
    "        else:\n",
    "            a = list(tree.keys())[0]\n",
    "            for i in range(len(self.featureNames)):\n",
    "                if self.featureNames[i] == a:\n",
    "                    break\n",
    "\n",
    "            try:\n",
    "                t = tree[a][datapoint[i]]\n",
    "                return self.classify(t, datapoint)\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "    def classifyAll(self, tree, data):\n",
    "        return [self.classify(tree, row) for row in data]\n",
    "\n",
    "    def make_tree(self, data, classes, featureNames, maxlevel=-1, level=0, forest=0):\n",
    "        \"\"\" The main function, which recursively constructs the tree\"\"\"\n",
    "        nData = len(data)\n",
    "        nFeatures = len(data[0])\n",
    "\n",
    "        try:\n",
    "            self.featureNames\n",
    "        except:\n",
    "            self.featureNames = featureNames\n",
    "\n",
    "        # List the possible classes\n",
    "        newClasses = []\n",
    "        for aclass in classes:\n",
    "            if newClasses.count(aclass) == 0:\n",
    "                newClasses.append(aclass)\n",
    "\n",
    "        # Compute the default class (and total entropy)\n",
    "        frequency = np.zeros(len(newClasses))\n",
    "\n",
    "        totalEntropy = 0\n",
    "        totalGini = 0\n",
    "        index = 0\n",
    "        for aclass in newClasses:\n",
    "            frequency[index] = classes.count(aclass)\n",
    "            totalEntropy += self.calc_entropy(float(frequency[index]) / nData)\n",
    "            totalGini += (float(frequency[index]) / nData) ** 2\n",
    "\n",
    "            index += 1\n",
    "\n",
    "        totalGini = 1 - totalGini\n",
    "        default = classes[np.argmax(frequency)]\n",
    "\n",
    "        if nData == 0 or nFeatures == 0 or (maxlevel >= 0 and level > maxlevel):\n",
    "            # Have reached an empty branch\n",
    "            return default\n",
    "        elif classes.count(classes[0]) == nData:\n",
    "            # Only 1 class remains\n",
    "            return classes[0]\n",
    "        else:\n",
    "\n",
    "            # Choose which feature is best\n",
    "            gain = np.zeros(nFeatures)\n",
    "            ggain = np.zeros(nFeatures)\n",
    "            featureSet = range(nFeatures)\n",
    "            if forest != 0:\n",
    "                np.random.shuffle(featureSet)\n",
    "                featureSet = featureSet[0:forest]\n",
    "            for feature in featureSet:\n",
    "                g, gg = self.calc_info_gain(data, classes, feature)\n",
    "                gain[feature] = totalEntropy - g\n",
    "                ggain[feature] = totalGini - gg\n",
    "\n",
    "            bestFeature = np.argmax(gain)\n",
    "            tree = {featureNames[bestFeature]: {}}\n",
    "\n",
    "            # List the values that bestFeature can take\n",
    "            values = []\n",
    "            for datapoint in data:\n",
    "                if datapoint[feature] not in values:\n",
    "                    values.append(datapoint[bestFeature])\n",
    "\n",
    "            for value in values:\n",
    "                # Find the datapoints with each feature value\n",
    "                newData = []\n",
    "                newClasses = []\n",
    "                index = 0\n",
    "                for datapoint in data:\n",
    "                    if datapoint[bestFeature] == value:\n",
    "                        if bestFeature == 0:\n",
    "                            newdatapoint = datapoint[1:]\n",
    "                            newNames = featureNames[1:]\n",
    "                        elif bestFeature == nFeatures:\n",
    "                            newdatapoint = datapoint[:-1]\n",
    "                            newNames = featureNames[:-1]\n",
    "                        else:\n",
    "                            newdatapoint = list(datapoint[:bestFeature])\n",
    "                            newdatapoint.extend(datapoint[bestFeature + 1:])\n",
    "                            newNames = list(featureNames[:bestFeature])\n",
    "                            newNames.extend(featureNames[bestFeature + 1:])\n",
    "                        newData.append(newdatapoint)\n",
    "                        newClasses.append(classes[index])\n",
    "                    index += 1\n",
    "\n",
    "                # Now recurse to the next level\n",
    "                subtree = self.make_tree(newData, newClasses, newNames, maxlevel, level + 1, forest)\n",
    "\n",
    "                # And on returning, add the subtree on to the tree\n",
    "                tree[featureNames[bestFeature]][value] = subtree\n",
    "\n",
    "            return tree\n",
    "\n",
    "    def printTree(self, tree, name):\n",
    "        if type(tree) == dict:\n",
    "            print(name, list(tree.keys())[0])\n",
    "            for item in list(list(tree.values())[0].keys()):\n",
    "                print(name, item)\n",
    "                self.printTree(list(tree.values())[0][item], name + \"\\t\")\n",
    "        else:\n",
    "            print\n",
    "            name, \"\\t->\\t\", tree\n",
    "\n",
    "    def calc_entropy(self, p):\n",
    "        if p != 0:\n",
    "            return -p * np.log2(p)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def calc_info_gain(self, data, classes, feature):\n",
    "\n",
    "        # Calculates the information gain based on both entropy and the Gini impurity\n",
    "        gain = 0\n",
    "        ggain = 0\n",
    "        nData = len(data)\n",
    "\n",
    "        # List the values that feature can take\n",
    "\n",
    "        values = []\n",
    "        for datapoint in data:\n",
    "            if datapoint[feature] not in values:\n",
    "                values.append(datapoint[feature])\n",
    "\n",
    "        featureCounts = np.zeros(len(values))\n",
    "        entropy = np.zeros(len(values))\n",
    "        gini = np.zeros(len(values))\n",
    "        valueIndex = 0\n",
    "        # Find where those values appear in data[feature] and the corresponding class\n",
    "        for value in values:\n",
    "            dataIndex = 0\n",
    "            newClasses = []\n",
    "            for datapoint in data:\n",
    "                if datapoint[feature] == value:\n",
    "                    featureCounts[valueIndex] += 1\n",
    "                    newClasses.append(classes[dataIndex])\n",
    "                dataIndex += 1\n",
    "\n",
    "            # Get the values in newClasses\n",
    "            classValues = []\n",
    "            for aclass in newClasses:\n",
    "                if classValues.count(aclass) == 0:\n",
    "                    classValues.append(aclass)\n",
    "\n",
    "            classCounts = np.zeros(len(classValues))\n",
    "            classIndex = 0\n",
    "            for classValue in classValues:\n",
    "                for aclass in newClasses:\n",
    "                    if aclass == classValue:\n",
    "                        classCounts[classIndex] += 1\n",
    "                classIndex += 1\n",
    "\n",
    "            for classIndex in range(len(classValues)):\n",
    "                entropy[valueIndex] += self.calc_entropy(float(classCounts[classIndex]) / np.sum(classCounts))\n",
    "                gini[valueIndex] += (float(classCounts[classIndex]) / np.sum(classCounts)) ** 2\n",
    "\n",
    "            # Computes both the Gini gain and the entropy\n",
    "            gain = gain + float(featureCounts[valueIndex]) / nData * entropy[valueIndex]\n",
    "            ggain = ggain + float(featureCounts[valueIndex]) / nData * gini[valueIndex]\n",
    "            valueIndex += 1\n",
    "        return gain, 1 - ggain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "\n",
    "    def __init__(self, n_trees: int = 128):\n",
    "        self.n_trees = n_trees\n",
    "        self.builder = DecisionTree()\n",
    "    \n",
    "    def subsample(self, dataset, ratio):\n",
    "        sample = []\n",
    "        n_sample = round(len(dataset) * ratio)\n",
    "        while len(sample) < n_sample:\n",
    "            index = randrange(len(dataset))\n",
    "            sample.append(dataset[index])\n",
    "        return sample\n",
    " \n",
    "    def fit(self, X, y, feature_names, sample_size):\n",
    "        trees = []\n",
    "        for n in range(self.n_trees):\n",
    "            subsample = self.subsample(X, sample_size)\n",
    "            tree = self.builder.make_tree(list(subsample), list(y), feature_names)\n",
    "            trees.append(tree)\n",
    "        self.trees = trees\n",
    "\n",
    "    def bagging(self, row):\n",
    "        predictions = [self.builder.classify(tree, row) for tree in self.trees]\n",
    "        print(\"predicitons for row \", row, \" :\", predictions)\n",
    "        return np.argmax(np.bincount(predictions))\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self.bagging(x) for x in X]\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data set\n",
    "\n",
    "Source: [Car Evaluation Data Set\n",
    "](https://archive.ics.uci.edu/ml/datasets/car+evaluation)\n",
    "\n",
    "**Class values:**\n",
    "unacc, acc, good, vgood\n",
    "\n",
    "**Attributes:**\n",
    "- buying:   vhigh, high, med, low.\n",
    "-\n",
    "maint:    vhigh, high, med, low.\n",
    "- doors:    2, 3, 4, 5more.\n",
    "- persons:  2, 4,\n",
    "more.\n",
    "- lug_boot: small, med, big.\n",
    "- safety:   low, med, high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = '~/Downloads/'\n",
    "\n",
    "attributes = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety']\n",
    "cars_dataset = pd.read_csv(DATASET_DIR+'car.data.txt')\n",
    "cars_dataset.columns = attributes + ['class']\n",
    "\n",
    "cars_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and predict\n",
    "\n",
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(cars_dataset)\n",
    "X = dataset[:,0:-1]\n",
    "y = dataset[:,-1]\n",
    "\n",
    "train_ratio = 0.8\n",
    "train_X, train_y = X[:int(X.shape[0] * train_ratio)], y[:int(y.shape[0] * train_ratio)]\n",
    "predict_X, predict_y = X[int(X.shape[0] * train_ratio):], y[int(y.shape[0] * train_ratio):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForest()\n",
    "\n",
    "# Train model on training data\n",
    "clf.fit(train_X, train_y, attributes, 1.0)\n",
    "# Predict result from testing data\n",
    "results = clf.predict(predict_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (result, target) in zip(results, predict_y):\n",
    "    print(result, target)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
